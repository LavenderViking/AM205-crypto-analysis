\documentclass[12pt,twoside]{article}

\usepackage{amsmath}
\usepackage{color}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{booktabs}
\graphicspath{ {images/} }
\usepackage{subfig}
\usepackage{placeins}
\usepackage{float}
\usepackage{mdframed}
\usepackage{booktabs}


\newcommand{\head}[1]{\textnormal{\textbf{#1}}}
\renewcommand{\contentsname}{Table of contents}

\newcommand{\cross}[1][1pt]{\ooalign{%
  \rule[1ex]{1ex}{#1}\cr% Horizontal bar
  \hss\rule{#1}{.7em}\hss\cr}}% Vertical bar

\input{macros}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

\newcommand{\theproblemsetnum}{}
\newcommand{\releasedate}{December 15, 2017}
\newcommand{\duedate}{December 15, 2017}
\newcommand{\tabUnit}{3ex}
\newcommand{\tabT}{\hspace*{\tabUnit}}

\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}





\begin{document}
\thispagestyle{empty}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\begin{center} % Center everything on the page
\vspace{5mm}
\includegraphics[scale=0.5]{Harvard}\\[3cm]
\textsc{\Large Advanced Scientific Computing: Numerical Methods}\\[0.5cm] % Major heading such as course name
\textsc{\large AM205}\\[2cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Do crypto currencies move in parallel manner?}\\[0.4cm] % Title of your document
\HRule \\[2.5cm]

%{\bf Name:} Yihang Yan, Tomas Gudmundsson, Nathaniel Stein


\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Students:}\\
Yihang Yan\\
Tomas Gudmundsson\\
Nathaniel Stein\\
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Professor:} \\
Chris Rycroft \\% Supervisor's Name
\hspace{1mm}\\
 \hspace{1mm} \\
\end{flushright}
\end{minipage}\\[2cm]

{\large \today}\\[2cm]
\end{center}



\newpage
\tableofcontents
\newpage
\setlength{\parindent}{0cm}
\section{Introduction}
This paper examines the statistical distribution of the price return patterns of cryptocurrencies through the lens of principal component analysis (PCA). The rise of cryptocurrencies is the most intensely debated paradigm shift in financial markets this past decade, with opinions ranging from JPMorgan CEO Jamie Dimon calling them a “fraud” to Bill Gates claiming “Bitcoin is better than currency.” To add to the unfolding drama, the Chicago Board Options Exchange (CBOE) debuted the trading of Bitcoin futures on December 10, 2017. A discussion on the merits and future of these innovations is beyond the scope of this paper.
\bigbreak
Instead, we will assume cryptocurrencies are not going away anytime soon and focus our analysis on other issues pertinent to investors and market-makers in cryptocurrencies. First, we will use PCA to assess whether the price returns of cryptocurrencies move in a parallel manner. Second, we sample different time frames to determine whether there have been any meaningful structural shifts in the price patterns of cryptocurrencies. Finally, we highlight the implications of our results for portfolio construction, risk management, and trading.
\bigbreak
This paper is structured as follows. The following section explains the motivation for applying PCA on financial asset returns and the mathematical theory behind the method before applying it to cryptocurrency price returns. Section 3 will use the results of these PCA findings to address the question of whether cryptocurrencies move in a parallel manner, and Section 4 will contextualize the significance of these findings by assessing whether the price patterns of cryptocurrencies have demonstrated significant inter-temporal changes. Our final section (Section 5) summarizes the conclusions of our analysis. Although PCA is routinely applied to studies of traditional asset classes and incorporated in various econometric models, to the best of this paper’s authors’ knowledge, it has not been applied in as rigorous of a fashion to cryptocurrency price returns.


\section{Principal Components Analysis}
\subsection{Motivation}

Before delving into the specifics of this paper’s implementation of PCA, let us motivate the benefits for studying asset returns with PCA. Principal components analysis (PCA) is a multivariate technique that analyzes a data in which observations are described by several inter-correlated quantitative dependent variables. This technique extracts the important information from the data to represent it as a set of new orthogonal variables called principal components, or factors.
\bigbreak
Factor models explaining stock returns and correlations have been very popular in finance. Unlike traditional factor models, the factors created by PCA do not usually have a clear economic interpretation. Rather, the components constructed in PCA are built to have special statistical characteristics whereby each component: 
\begin{itemize}
	\item Aims to account for as much of variation in the data as possible.
	\item Is uncorrelated with every other component, i.e., the components are orthogonal to each other.
\end{itemize}

In short, for the purposes of this paper, PCA enables the identification of the underlying statistical factors that cause the comovement in cryptocurrency returns. These findings can be used to quantify the relative importance of each cryptocurrency in explaining the movement of the cryptocurrency market as a whole.
\bigbreak
The first principal component is the component that best explains variation in the underlying data, i.e., the greatest amount of variation, and is of particular importance to this study. In finance, risk is frequently broken down into two categories: \textit{systematic} (i.e., market) risk that can be mitigated through a diversified portfolio and \textit{idiosyncratic} risk that is specific to each individual asset and cannot be diversified away. In applications of PCA on asset returns, the first principal component is generally accepted as a representation of the overall return of the assets, arguably representing the return to investors for taking on the systematic risk of those assets (Shukla and Trzcinka, 1990). In other words, if all the assets shared the same idiosyncratic risks, the first principal component could be conceptualized as the “equal weighted market index” (Shukla and Trzcinka, 1990). Thus, for assets that are fairly correlated with one another, i.e., a large proportion of their comovement is accounted for by the general fortunes (or misfortunes) in their market, one would expect the first component to account for a relatively large proportion of variance and to have similar loadings on the variables.
\bigbreak
Mathematically, PCA depends upon the eigendecomposition of positive semi-definite matrices and upon the singular value decomposition (SVD) of rectangular matrices.

\subsection{Mathematics of Principal Components}

\subsubsection{Singular Value Decomposition (SVD)}
\bigbreak
Any real symmetric $m \times m$ matrix A has a spectral decomposition of the form,
\begin{equation}
A = U\triangle U^{T}
\end{equation}
\bigbreak
where $U$ is an orthonormal matrix (matrix of orthogonal unit vectors: $U_{T}U = I$ or $\sum_{k}U_{ki}U_{kj} = \delta_{ij}$) and $\triangle$ is a diagonal matrix. The columns of $U$ are the eigenvectors of matrix $A$ and the diagonal elements of $\triangle$ are the eigenvalues. If $A$ is positive-definite, the eigenvalues will all be positive. Multiplying with $U$, equation 1 can be re-written to,
\begin{equation}
AU = U\triangle U^{T}U = UA
\end{equation}
\bigbreak
This can be written as a normal eigenvalue equation by defining the $i$th column of $U$ as
$u_{i}$ and the eigenvalues as $\lambda_{i} = \triangle_{ii}$:
\begin{equation}
Au_{i} = \lambda_{i}u_{i}
\end{equation}
\bigbreak
Let's look at more general case. An unsymmetrical (n x m) matrix, where $n \geq m$ B has the decomposition,
\begin{equation}
X = U\triangle V^{T}
\end{equation}

where U is a n x m matrix with orthonormal columns ($U^{T}U = I$), while V is a m x m orthonormal matrix ($V_{T}V = I$), and $\triangle$ is a m × m diagonal matrix with positive or zero elements, called the singular values.
\bigbreak
From B we can construct two positive-definite symmetric matrices, $BB^{T}$ and $B^{T}B$, each of which we can decompose
\begin{equation}
BB^{T} = U\triangle V^{T}V\triangle U^{T} = U\triangle^2U^{T}
\end{equation}
\begin{equation}
B^{T}B = V\triangle^2V^{T}
\end{equation}
\bigbreak
We can now show that $BB^{T}$ which is n x n and $B^{T}B$ which is m × m will share m eigenvalues and the remaining n - m eigenvalues of $BB^{T} $ will be zero.
\bigbreak
Using the decomposition above, we can identify the eigenvectors and eigenvalues for $BB_{T}$ as the columns of V and the squared diagonal elements of $\triangle$ , respectively. Denoting one such eigenvector by v and the diagonal element by $\gamma$, we have:

\begin{equation}
B^{T}Bv = \gamma^2v
\end{equation}
\begin{equation}
BB^{T}Bv = \gamma^2Bv
\end{equation}

\bigbreak
This means that we have an eigenvector $u = Bv$ and eigenvalue $\gamma^2$ for $BB^{T}$ as well, since:
\begin{equation}
(BB^{T})Bv = \gamma^2Bv
\end{equation}

\bigbreak
We have now shown that $B^{T}B$ and $BB^{T}$ share m eigenvalues.
\bigbreak
In order to prove that the remaining n − m eigenvalues of $BB_{T}$ is zero. We need to consider an eigenvector for  $BB^{T}$ , $u_{\perp}$: $BB^{T} u_{\perp} = \beta_{\perp} u_{\perp}$ which is orthogonal to the m eigenvectors $u_{i}$ already determined, i.e. $U^{T} u_{\perp} = 0$. Using the decomposition $BB^{T} = U\triangle^2U^{T}$, we immediately see that the eigenvalues $\beta_{\perp}$ must all be zero,
\begin{equation}
BB^{T} u_{\perp} = U\triangle^2U^{T} u_{\perp} = 0 u_{\perp}
\end{equation}
\bigbreak
\subsubsection{Principal component analysis (PCA) by SVD}
\bigbreak
We denote the matrix of eigenvectors sorted according to eigenvalue by $\hat{U}$ and we can then PCA transformation of the data as $Y = \hat{U}^{T}X$. The eigenvectors are called the principal components. By selecting the first d rows of $Y$, we can project the data from $n$ down to $d$ dimensions.
\bigbreak
We decompose $X$ using SVD, i.e.
\begin{equation}
X = U\triangle V^{T}
\end{equation}
\newline
and find that we can write the covariance matrix as
\begin{equation}
C = \frac{1}{n} XX^{T} = \frac{1}{n} U\triangle^2U^{T}
\end{equation}
\bigbreak
Following from the fact that SVD routine order the singular values in descending order we know that, if $n < m$, the first n columns in $U$ corresponds to the sorted eigenvalues of $C$ and if $m \geq n$, the first m corresponds to the sorted non-zero eigenvalues of $C$. The transformed data can thus be written as:
\begin{equation}
Y = \hat{U}^{T}X = \hat{U}^{T}U\triangle V^T
\end{equation}

where $\hat{U}^{T}U$ is a simple n x m matrix which is one on the diagonal and zero everywhere else. So we can write the transformed data in terms of the SVD decomposition of $X$. 


\subsubsection{Finding the components}

In PCA, the components are obtained from the singular value decomposition of the dataset $X$. Specifically, with $X = U\triangle V^{T}$ (equation 1), the matrix of factor scores, denoted $F$ is obtained as
\begin{equation}
F = U\triangle
\end{equation}

The matrix $V$ gives the coefficients of the linear combinations used to compute the factors scores. This matrix can also be interpreted as a projection matrix because multiplying $X$ by $V$ gives the values of the projections of the observations on the principal components. This can be shown as:
\begin{equation}
F = U\triangle = U\triangle VV^{T}  = XV
\end{equation}
\bigbreak
The components can be represented geometrically by the rotation of the original axes. Each of these components will be linear combinations of the observed variables we have in our data, and will be orthogonal to each other. That is, each component is independent of each other, and variation in one is unrelated to variation in another. 

\subsubsection{Contribution of an observation to a component}

The eigenvalue associated to a component is equal to the sum of the squared factor scores for this component. Therefore, the importance of an observation for a component can be obtained by the ratio of the squared factor score of this observation by the eigenvalue associated with that component. This ratio is called the contribution of the observation to the component. Formally, the contribution of observation i to component l is denoted $ ctr_{i,l}$, it is obtained as:
\begin{equation}
ctr_{i,l} = \frac{f^2_{i,l} }{\sum f^2_{i,l}} = \frac{f^2_{i,l} }{\lambda_{l}}
\end{equation}
where $\lambda_{l}$ is the eigenvalue of the $l$th component. The value of a contribution is between 0 and 1 and, for a given component, the sum of the contributions of all observations is equal to 1. The larger the value of the contribution, the more the observation contributes to the component. 


\section{Jacobi eigenvalue algorithm}

In order to find eigenvalues and eigenvectors of the covariance matrix, A, we use the Jacobi method. The method works by finding the largest non-diagonal element at location $(i,j)$ and makes it zero by doing a plane rotation on rows and columns $i$ and $j$. The Jacobi method is quite efficient with quadratic convergence and can be parallelized easily.  \\


The rotation for matrix A works as follows:
\begin{equation}
     A' = P_{i,j,\theta}^T \cdot A \cdot P_{i,j,\theta}  
\end{equation}

where $P_{i,j,\theta} = P(i,j,\theta)$ is a Givens rotation matrix with the form:

\begin{equation}
P(i,j,\theta) = 
\begin{bmatrix}
     1 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\
     \vdots & \ddots & \vdots &   & \vdots &  & \vdots \\
          0 & \cdots & c & \cdots & s & \cdots & 0 \\
         \vdots &  & \vdots & \ddots  & \vdots &  & \vdots \\
     0 & \cdots & -s & \cdots & c & \cdots & 0 \\
              \vdots &  & \vdots &  & \vdots & \ddots  & \vdots \\
                   0 & \cdots & 0 & \cdots & 0 & \cdots & 1 \\
\end{bmatrix}
\end{equation}\\

This matrix has ones on the diagonal except where $c=cos(\theta)$ at locations (i,i) and (j,j). All other elements are 0 except $s=sin(\theta)$ at locations (i,j) and (j,i).\\

When the Givens rotation matrix, $P_{i,j,\theta}$,  is multiplied with another matrix, $A$, as $P\cdot A$ it simulates a clockwise rotation in the plane by an angle $\theta$ in order to make the element at location $(i,j)$ zero and only affects rows and columns $i$ and $j$ in the process.\\
 

In order to figure out the values of $c,s$ and $\theta$ we will simulate a matrix multiplication with $2x2$ matrices that contain the relevant information. We will represent the matrices as follows:
\begin{equation}
P_{i,j,\theta} = 
\begin{bmatrix}
     c_{ii} & s_{ij} \\
    -s_{ji} & c_{jj} \\
\end{bmatrix}
,A = 
\begin{bmatrix}
     A_{ii} & A_{ij} \\
    A_{ji} & A_{jj} \\
\end{bmatrix}
,A' = 
\begin{bmatrix}
     A_{ii}' & A_{ij}' \\
    A_{ji}' & A_{jj}' \\
\end{bmatrix}
\end{equation}

where the notation $A_{ij}$ denotes element at location $(i,j)$ in matrix A.\\


Now we will find $c, s$ and $\theta$ so that the matrix multiplication zeros the largest element. In order for P to satisfy eq. 17 we expand the matrix multiplication as follows with $c_{ii}=c_{jj}=c$ and $s_{ji}=s_{ij}=s$:\\
\begin{equation}
\begin{split}
\begin{bmatrix}
     A_{ii}' & A_{ij}' \\
    A_{ji}' & A_{jj}' \\
\end{bmatrix}
 &=
\begin{bmatrix}
     c & -s \\
    s & c \\
\end{bmatrix}
\cdot
\begin{bmatrix}
     A_{ii} & A_{ij} \\
    A_{ji} & A_{jj} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
     c & s \\
     -s & c \\
\end{bmatrix}\\
&=\begin{bmatrix}
     c\cdot A_{ii} - s\cdot A_{ji} & c\cdot A_{ij} - s\cdot A_{jj} \\
     s\cdot A_{ii} + c\cdot A_{ji} & s\cdot A_{ij} + c\cdot A_{jj} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
     c & s \\
     -s & c \\
\end{bmatrix}\\
&=\begin{bmatrix}
c^2\cdot A_{ii} - cs\cdot A_{ji} - cs\cdot A_{ij} + s^2\cdot A_{jj} & cs\cdot A_{ii} - s^2\cdot A_{ji} + c^2\cdot A_{ij} - cs\cdot A_{jj}\\
cs\cdot A_{ii} + c^2\cdot A_{ji} - s^2\cdot A_{ij} - cs\cdot A_{jj} & s^2\cdot A_{ii} + cs\cdot A_{ji} + cs\cdot A_{ij} + c^2\cdot A_{jj}\\
\end{bmatrix}\\
&=\begin{bmatrix}
c^2\cdot A_{ii} - 2cs\cdot A_{ij}  + s^2\cdot A_{jj} & (c^2-s^2)\cdot A_{ij}  + cs\cdot (A_{ii} - A_{jj})\\
(c^2-s^2)\cdot A_{ij}  - cs\cdot (A_{ii} - A_{jj}) & c^2\cdot A_{jj} + 2cs\cdot A_{ij} + s^2\cdot A_{ii}\\
\end{bmatrix}
\end{split}
\end{equation}

where the last elements are simplified because $A_{ij}=A_{ji}$.\\



In order to make the non diagonal element in this matrix as 0 we will examine the non diagonal equation as follows:
\begin{equation}
A'_{ij} = (c^2 - s^2) \cdot A_{ij} + cs(A_{ii}-A_{jj}) = 0
\end{equation}


Hence it follows that:
\begin{equation}
\frac{c^2-s^2}{cs} = \frac{A_{jj}-A_{ii}}{A_{ij}}
\end{equation}

and we can define the rotation angle as follows:
\begin{equation}
\theta = cot(2\phi) = \frac{c^2-s^2}{2cs} = \frac{A_{jj}-A_{ii}}{2A_{ij}}
\end{equation}
and by letting $t = s/c$ we can rewrite the equation above as:\\
\begin{equation}
2cs\theta = c^2 - s^2 <=>
t^2 + 2t\theta - 1 = 0
\end{equation}
which has the solutions:\\
\begin{equation}
t = 
\bigg\{
  \begin{tabular}{cc}
$ -\theta + \sqrt{\theta^2+1}$ \\
$ -(\theta + \sqrt{\theta^2+1})$ \\
  \end{tabular}
\end{equation}
These solutions can be written more succinctly as
\begin{equation}
t =  -\theta + \sqrt{\theta^2+1}  = \frac{ \left(-\theta + \sqrt{\theta^2+1}\right)\left(-\theta + \sqrt{\theta^2+1}\right) }{\theta + \sqrt{\theta^2+1}}
= \frac{ -\theta^2 + \theta^2 + 1 }{\theta + \sqrt{\theta^2+1}} = \frac{1}{\theta + \sqrt{\theta^2+1}}
\end{equation}
\begin{equation}
t =  -\theta - \sqrt{\theta^2+1}  = \frac{ \left(-\theta - \sqrt{\theta^2+1}\right)\left(\theta - \sqrt{\theta^2+1}\right) }{\theta - \sqrt{\theta^2+1}}
= \frac{ -\theta^2 + \theta^2 + 1 }{\theta - \sqrt{\theta^2+1}} = \frac{-1}{-\theta + \sqrt{\theta^2+1}}
\end{equation}

We want to rotate the matrix by the angle which corresponds to the smaller root of this equation and generally we can write the smaller root as:

\begin{equation}
t =  \frac{sign(\theta)}{|\theta|+ \sqrt{1+\theta^2}}
\end{equation}
and since $t=s/c$ we now have:\\
\begin{equation}
c =  \frac{1}{\sqrt{t^2+1}}, \hspace{5mm} s = t\cdot c
\end{equation}


Now we know how to set these variables for the rotations to work and we need to look at three scenarios to update the matrix when a rotation is performed:

\begin{enumerate}[label=\roman*)]
  \item Set value at location (i,j) as 0
  \item Change diagonal values at locations (i,i) and (j,j)
  \item Change values on rows and columns i and j except (i,i) and (j,j)
\end{enumerate}

The following diagram shows which values are modified during the rotation:
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{matrix.png}
\caption{Visual representation of how the Jacobi plane rotations work}
\end{center}
\end{figure}
We define a tolerance $tol=10^{-9}$ and perform rotations until the largest non-diagonal element is less than the tolerance. Since the matrix is symmetric we will perform rotations on the upper triangle of the matrix until it has converged.\\


For scenario i) we simply set the value of $A_{ij}'$ as 0. However, for scenario ii) we will look at the top left and bottom right elements in eq. 20 to gather equations to set the diagonal elements $A_{ii}'$ and $A_{jj}'$ . We have:
\begin{equation}
A_{ii}' = c^2 \cdot A_{ii} - 2cs\cdot A_{ij} + s^2\cdot A_{jj}
\end{equation}

From eq. 21 (because $A_{ij}'=0$) we can isolate $A_{jj}$ as\\
\begin{equation}
A_{jj} = A_{ii} - A_{ij}\frac{s^2-c^2}{cs}
\end{equation}
and since $c^2+s^2=1$ we simplify eq 14. as:
\begin{equation}
\begin{split}
A_{ii}' &= c^2 \cdot A_{ii} - 2cs\cdot A_{ij} + s^2\cdot A_{jj}\\
& = c^2 \cdot A_{ii} - 2cs\cdot A_{ij} + s^2 \left(A_{ii} - A_{ij}\frac{s^2-c^2}{cs}     \right)\\
& = (c^2 + s^2) \cdot A_{ii} - s\left(2c +  \frac{s^2-c^2}{c}      \right)A_{ij}\\
&= (c^2 + s^2) \cdot A_{ii} - \frac{s}{c}\left(2c^2 +  s^2-c^2      \right)A_{ij}\\
& = A_{ii} - \frac{s}{c}\left(c^2 +  s^2      \right)A_{ij}\\
& = A_{ii} - t\cdot A_{ij} 
\end{split}
\end{equation}
Similarly we have:\\
\begin{equation}
A_{jj}' = A_{jj} + t\cdot A_{ij}
\end{equation}



For scenario iii) we can look at top of eq 4. and note that if we consider an element $A_{rj}$ when we perform rotation around $A_{ij}$ that only the last two matrices will change the result since the first matrix changes rows i and j and does not have effect on row $r$. The last matrix changes columns i and j and therefore changes the resulting matrix. Multiplying through these matrices gives us the equations:
\begin{equation}
\bigg\{
  \begin{tabular}{cc}
$A_{ri}' = cA_{ri} - sA_{rj}$\\
$A_{rj}' = cA_{ri} + sA_{rj}$\\
  \end{tabular}
\end{equation}\\

Lets look at $A_{ri}'$ which can be represented as:
\begin{equation}
\begin{split}
A_{ri}' &= cA_{ri} - sA_{rj}\\
&= \left(1 - \frac{(1-c)(1+c)}{1+c} \right) A_{ri} - sA_{rj} \\
&= \left(1 - \frac{1-c^2}{1+c} \right) A_{ri} - sA_{rj} \\
&= \left(1 - \frac{s^2}{1+c} \right) A_{ri} - sA_{rj} \\
&= A_{ri}  - s \left(A_{rj} + \frac{s}{1+c} A_{ri} \right) \\
&= A_{ri}  - s \left(A_{rj} + \tau A_{ri} \right)  
\end{split}
\end{equation}

where 
\begin{equation}
\tau = \frac{s}{1+c}
\end{equation}

which has less roundoff error than eq. 34.\\

By performing the Jacobi rotations until the matrix converges we will end up with the eigenvalues at the diagonals. To get the eigenvectors for the eigenvalues we will initialise an identity matrix, $P$, and each time we update the matrix $A$ we will update $P$ accordingly. The eigenvector for each eigenvalue will be the same column it belongs to.\\



Similarly we have
\begin{equation}
A_{rj}' = A_{rj}  + s \left(A_{ri} - \tau A_{rj} \right) 
\end{equation}

\vspace{5mm}
To summarise we set values of elements in rows r and l and columns r and l as follows:
\begin{enumerate}[label=\roman*)]
  \item $A_{ij}=0$
\item $\bigg\{
  \begin{tabular}{cc}
$A_{ii}' = A_{ii} - t\cdot A_{ij}$  \\
$A_{jj}' = A_{jj} + t\cdot A_{ij}$ \\
  \end{tabular}$
\item $\bigg\{
  \begin{tabular}{cc}
$A_{ri}' = A_{ri}  - s \left(A_{rj} + \tau A_{ri} \right)$   \\
$A_{rj}' = A_{rj}  + s \left(A_{ri} - \tau A_{rj} \right)$   \\
  \end{tabular}$
\end{enumerate}

where
\[ s=t\cdot c, \hspace{5mm} t = \frac{sign(\theta)}{|\theta|+ \sqrt{1+\theta^2}}, \hspace{5mm} \tau = \frac{s}{1+c}, \hspace{5mm} \theta = \frac{A_{ii}-A_{jj}}{2A_{ij}}  \]

\section{Implementation}
\subsection{Data}
Our raw dataset consists of the closing prices for the top seven cryptocurrencies in terms of market capitalization (as listed on CoinMarketCaps' historical tables) for the period between August 7, 2015 to November 7, 2017. The start of the period is the first day for which a closing price on all seven of the selected cryptocurrencies was available because some, like Ripple, did not come into existence until much after the first cryptocurrencies, like Bitcoin. Using this dataset, we first created a time-series matrix containing rolling month-to-month changes in the closing prices for these cryptocurrencies, i.e., rolling monthly returns. For example, the return for Bitcoin (btc) on September 6, 2015 is equal to the percentage change in its closing price from August 7, 2015 (279.58) to September 6, 2015 (239.84): $239.84/279.58 - 1 = -14.21\%$.

\bigbreak
We then standardized the returns for each cryptocurrency by centering around that particular currency's mean return and scaling by its standard deviation to achieve unit variance. The correlation matrix of this standardized dataset serves as the input for PCA. The correlation matrix is typically used instead of the covariance matrix. However, the eigendecomposition of a covariance matrix formed on standardized data yields the same results as an eigendecomposition on a correlation matrix (whether done on standardized or unstandardized data), since the correlation matrix can be understood as the normalized covariance matrix. When using a covariance matrix on unstandardized financial time series data, it should be pointed out that the resulting PCA could lead to the first principal component being dominated by the asset with the largest variance. This is especially impaction when the variances of the individual assets are significantly different from one another.

\bigbreak
\newpage
\begin{figure}[H]
\begin{center}
\includegraphics[scale=.7]{corr.png}
\caption{The correlation matrix of the crypto currencies}
\end{center}
\end{figure}

\bigbreak

One of the basic assumptions behind using PCA is that the variables being examined have some kind of a linear relationship with each other; else, it would be unlikely that they share any meaningful common components. To assess the strength of the variables’ linear relationship and, consequently, suitability for PCA, one can use the correlation coefficients between the pairs of the variables (Hair, 2010). We conclude that the correlation between cryptocurrency pairs is strong enough to at least warrant further exploration.

\subsection{Computing eigenvectors and corresponding eigenvalues}
\bigbreak
We will use the Jacobi method to find eigenvalues and eigenvectors of the correlation matrix since this method is a fairly robust way to extract all of the eigenvalues and eigenvectors of a symmetric matrix. The method is based on a series of rotations, called Givens rotations, which are chosen to eliminate off-diagonal elements while preserving the eigenvalues. The algorithm finds the maximum non-diagonal element at index $(i,j)$ by performing a scan over the matrix, zeros it out and then performs a plane rotation that affects elements in rows and columns $i$ and $j$. During each modification the algorithm also modifies a transformation matrix in the same way in order to keep track of the eigenvectors. This algorithm has a quadratic convergence and finishes when all non-diagonal elements are less than a chosen tolerance value. The eigenvalues are located at the diagonal of the input matrix and the eigenvectors are the columns of the transformation matrix. Further details of Jacobi method are covered in Section 3.\\

We can check if the eigenvector-eigenvalue calculation is correct by using the equation:

$$M_{cov} u_i = \lambda_i u_i$$ 
where
\begin{center}
$M_{cov}$ = covariance matrix, \\
$u_i$ = ith colummn of eigenvector matrix, \\
$\lambda_i$ = eigenvalue associated with $u_i$\\
\end{center}
and by testing it this way and against built in svd packages we have found that it gives the same results.

\subsection{Sorting Eigenpairs and Explained Variance}

In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest corresponding eigenvalues bear the least information about the distribution of the data; those are the ones that can be dropped. In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top $k$ eigenvectors. The first principal component is required to have the largest possible variance. The second component is computed under the constraint of being orthogonal to the first component and to have the largest possible inertia. The other components are computed likewise.
\bigbreak
\begin{table}[H]
\begin{tabular}{ccc}
\hline
\head{Principal Component} & \head{Eigenvalue} & \head{Eigenvector}\\
\hline
1  & 2.654 & array([ 0.517,  0.334,  0.358,  0.455,  0.137,  0.487,  0.174])\\
2 & 1.376 &  array([ 0.237,  0.15 , -0.508,  0.309, -0.327,  0.063, -0.676])\\
3 & 0.941 & array([-0.105,  0.305, -0.204, -0.065,  0.88 , -0.037, -0.274])\\
4 & 0.769 & array([-0.286,  0.867,  0.007, -0.226, -0.311, -0.044,  0.13 ])\\
5 & 0.564 & array([-0.015, -0.095,  0.228, -0.611, -0.056,  0.64 , -0.39 ])\\
6 & 0.464 & array([-0.013, -0.064, -0.712, -0.087,  0.027,  0.472,  0.508])\\
7 & 0.239 & array([ 0.764,  0.091, -0.118, -0.511,  0.012, -0.351,  0.103])\\
\hline
\end{tabular}
\caption{Eigenvalue and Eigenvector for each principal component}
\end{table}
\bigbreak
After sorting the eigenpairs, the next question is “how many principal components are we going to choose for our new feature subspace?” A useful measure is the proportion of variance, which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components.
\bigbreak
Table 2 displays the results of a principal-component analysis of the cryptocurrencies’ daily returns: The eigenvalues, proportio of variance, and cumulative variance for each component. The single-strongest factor only explains 37.9$\%$ of the variation of crypto-currency returns. Moreover, each subsequent factor is providing only slowly declining additional information content, so that at least 5 factors are needed in order to account for 90$\%$ of the variation from these 7 crypto-currencies. 
\bigbreak
\begin{table}[H]
\begin{tabular}{cccc}
\hline
\head{Principal Component} & \head{Eigenvalue} & \head{Proportion of Variance} & \head{Cumulative Variance}\\
\hline
1 & 2.654 &  37.915\% & 37.915\%\\
2 & 1.376 & 19.635\% & 57.55\%\\
3 & 0.941 &  13.422\% & 70.972\%\\
4 & 0.769 &  10.967\% & 81.939\%\\
5 & 0.564 &  8.040\%  & 89.979\%\\
6 & 0.464 & 6.613\% & 96.592\%\\
7 & 0.239 &  3.408\% & 100\%\\
\hline
\end{tabular}
\caption{Explained variance and cumulative variance for each principal component}
\end{table}
\bigbreak

\subsection{Component Loadings}
\bigbreak
In multivariate space, the correlation between the principal component and the original variables (cryptocurrency price returns) is called the loading (or weight) of that component on the original variable. Based on loadings, we can tell how much of the variation in a variable is explained by the component.
\bigbreak
Loadings = Orthonormal Eigenvectors $* \sqrt{Absolute Eigen values}$
\bigbreak
Principal component loading diagram is shown below:
\begin{figure}[H]
\begin{center}
\includegraphics[scale=.6]{pca_loadings.png}
\end{center}
\caption{PCA matrix for 7 crypto currencies}
\end{figure}
\begin{table}[H]
\begin{tabular}{cccccccc}
\hline
\head{Principal Component} & \head{Ripple} & \head{Bitcoin} & \head{Litecoin} &\head{Monero} &\head{Nem} &\head{Dash} &\head{Ethereum}\\
\hline
1 & 0.844&0.544&0.583&0.742&0.224&0.793&0.284\\
2 & 0.278&0.176&-0.595&0.363&-0.383&0.074&-0.793\\
3 & -0.102	&0.295&-0.198&-0.063&0.853&-0.036&-0.266\\
4 & -0.250&0.760&0.006&-0.198&-0.272&-0.039&0.114\\
5 & -0.011	&-0.071&0.171&-0.459&-0.042&0.481&-0.293\\
6 & -0.009&-0.043&-0.485&-0.059&0.018&0.321&0.346\\
7 & 0.373&0.044&-0.058&-0.250&0.006&-0.171&0.051\\
\hline
\end{tabular}
\caption{Component Loadings}
\end{table}

\bigbreak
The first principal component is strongly correlated (loading score $\geq$ 0.7) with three out of seven currencies. The first principal component increases with increasing Ripple, Monero, and Dash scores. This suggests that these three currencies are likely to vary together (positively correlated). If one increases, then the remaining ones tend to as well. Furthermore, we see that the correlation of first principal component with those six currencies are quite similar. \newline
We also notice that the second principal component (negatively) correlates most strongly with Ethereum. In fact, we could state that based on the correlations of -0.793, this principal component is primarily a measure of Ethereum.
\bigbreak
We construct the bi-plot of relative weights of each cryptocurrency in the first two PC components (PC-1 and PC-1) arising from the previous analysis:
\bigbreak
\begin{figure}[H]
\begin{center}
\includegraphics[scale=.7]{biplot1.png}
\caption{Bi-plot of relative weights for each crypto currency in the first two PC components}
\end{center}
\end{figure}
\bigbreak
This diagram also demonstrate the distinct movement between Ethereum and the rest of cryptocurrencies. All seven variables have positive values in the PC1 axis, while Litecoin, Nem, and Ethereum are negative in PC2's and Ripple, Bitcoin, Monero and Dash is positive. Since all the variables are positive in PC1, those which constrain the system the most are Ripple and (then) Dash and Monero (in PC1 axis).The PC2, which has much smaller variance, contrasts Ethereum from everything else.
\bigbreak
The cosine of the angles between vectors is equal to the correlation between those variables. Hence vectors pointing in the same direction are perfectly correlated, and those at right angles are uncorrelated. As we can see, Ripple and Bitcoin are high correlation, while Ethereum has extreme low correlation with Bitcoin, Monero, and Ripple. This conclusion is also supported by the correlation matrix.

\bigbreak

In order to further confirm the weak correlation for each pair of currencies (cryptocurrency time-series), we make use of two distinct tools, namely, one-factor linear regression (hence its $R^2$ metric) and Kendall’s rank correlation metric of $\tau$. Specifically, Kendall's $\tau$ is calculated based on concordant and discordant pairs. It's insensitive to error and its P values are more accurate with smaller sample sizes. Correlation analyses can be used to test for associations in hypothesis testing.  The null hypothesis is that there is no association between the variables under study.  Thus, the purpose is to investigate the possible association in the underlying variables.
\bigbreak
\begin{figure}[H]%
    \centering
    \subfloat{{\includegraphics[scale=.2]{btc_ltc}}}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{btc_nem} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{btc_rip} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{dash_btc} }}%
\end{figure}
\begin{figure}[H]%
    \centering
    \subfloat{{\includegraphics[scale=.2]{dash_ltc}}}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{dash_mon} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{dash_nem} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{dash_rip} }}%
\end{figure}
\begin{figure}[H]%
    \centering
    \subfloat{{\includegraphics[scale=.2]{eth_btc}}}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{eth_dash} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{eth_ltc} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{eth_mon} }}%
\end{figure}
\begin{figure}[H]%
    \centering
    \subfloat{{\includegraphics[scale=.2]{eth_nem}}}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{eth_rip} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{ltc_rip} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{mon_btc} }}%
\end{figure}
\begin{figure}[H]%
    \centering
    \subfloat{{\includegraphics[scale=.2]{mon_ltc}}}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{mon_rip} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{nem_ltc} }}%
    \qquad
    \subfloat{{\includegraphics[scale=.2]{nem_rip} }}%
    \caption{Linear correlations for such pair of crypto currencies }
\end{figure}

\begin{table}[H]
\begin{tabular}{cccc}
\hline
\head{Currencies Pair} & \head{r2 - p value} & \head{tau - p value} \\
\hline
btc - ltc & 0.322 - $1.259e^{-20}$ & 0.364 - $4.667e^{-53}$\\
btc - nem & 0.349 - $4.211e^{-24}$ & 0.193 - $4.066e^{-16}$\\
btc - rip & 0.305 - $1.631e^{-18}$ & 0.146 - $6.484e^{-10}$\\
dash - btc & 0.031 - 0.382 & 0.006 - 0.797\\
dash - ltc & 0.018 - 0.608 & 0.082 - 0.001\\
dash - mon & 0.128 - 0.0001 & 0.314 - $5.295e^{-40}$\\
dash - nem & 0.133 - 0.0001 & 0.237 - $1.761e^{-23}$\\
dash - rip & 0.037 - 0.298 & 0.077 - 0.001\\
eth - btc & 0.164 - $3.196e^{-6}$ & 0.155 - $5.608e{-11}$\\
eth - dash & 0.470 - $6.202e^{-45}$ & 0.368 - $2.346e^{-54}$\\
eth - ltc & 0.192 - $5.180e^{-8}$ & 0.198 - $7.079e^{-17}$\\
eth - mon & 0.171 - $1.169e^{-6}$ & 0.317 - $7.959e^{-41}$\\
eth - nem & 0.361 -$8.235e^{-26}$ & 0.312 - $1.714e^{-39}$\\
eth - rip & 0.325 - $4.846e^{-21}$ & 0.321 - $1.164e^{-41}$\\
ltc - rip & 0.694 - $4.391e^{-115}$ & 0.198 - $5.890e{-17}$\\
mon - btc & 0.102 - 0.004 & 0.159 - $1.980e^{-11}$\\
mon - ltc & 0.044 - 0.220 & 0.133 - $1.840e^{-8}$\\
mon - rip & 0.066 - 0.062 & 0.238 - $8.575e^{-24}$\\
nem - ltc & 0.428 - $1.1262e^{-36}$ & 0.121 - $3.055e^{-7}$\\
nem - mon & 0.114 - 0.001  & 0.286 - $1.637e^{-33}$\\
nem - rip & 0.630 -$4.111e^{-89}$ & 0.233 $8.714e^{-23}$\\
\hline
\end{tabular}
\caption{$R^2$  metric and Kendall's rank correlation metric of $\tau$ for each pair of crypto currencies}
\end{table}


\bigbreak
As we can see, indeed, a low degree in linear relationship is confirmed by $R^2 \leq 0.9$ at $\tau \leq 0.75$ for all of them.
\bigbreak

The purpose of this section was to elucidate how PCA can be conducted on a financial time series like the universe of cryptocurrencies that are the focus of this paper. We have shown what the eigenvectors, components and loadings look like. Yet, we have not provided any point of reference against which to contextualize these results. This will be done in the next section, where we aim to answer the question of whether cryptocurrencies move in a parallel manner. To answer such a question, one needs to define what return patterns would justify being considered "parallel.". To do this, we will employ statistical tests done in other PCA studies on financial time series data and provide a point of reference to help compare and contrast the results of PCA on cryptocurrencies against another asset class.

\section{Do Cryptocurrencies Move in a Parallel Manner?}
On November 23, 2017, Fortune magazine ran the story titled "Ethereum Price Hits New High as Billionaire Predicts 25\% Surge In the Next Month." Several articles like this appear daily for seemingly each cryptocurrency, and each touts the purported benefits of its subject coin's features over the rest. This all alludes to a question common in the investment world for all groupings of assets. If Ethereum does well in a given month or year, is it because \textit{Ethereum} is special in relation to its coin colleagues or is it because cryptocurrencies \textit{as a whole} did well? In more statistical words, how much of the variation in individual cryptocurrencies could be described by variation in the universe of cryptocurrencies as a whole? And relatedly, is it useful to aggregate cryptocurrencies into an "asset class" or grouping? Before answering this ourselves, we review prior related studies on the matter.
\subsection{Existing Literature}
Broadie (2012) conducts PCA on the price returns of assets that are generally accepted to rise and fall in a parallel fashion as a result of changes in prevailing and expected future interest rates: on-the-run Treasury securities of varying maturities. He finds that not only does the first principal component account for an overwhelming majority of the variation (95\%), but also that the loadings of the first component are near-equal with respect to each individual Treasury security. Of course, these findings are not surprising given that the most significant risk to the price of Treasury securities, i.e., cause for variation in their price returns, is a change in the interest rate curve the effect it has on the discounting of cash flows. Unlike with Treasuries, the causes for variation in the return on equities are arguably more nuanced. Feeney and Hester (1967), in one of the first studies of its kind, conduct PCA on the rates of return of the 30 Dow Jones industrial (DJI) stocks over a 50 quarter period and find that the first component accounts for 41\% of the variance (the second component accounts for 9\%) and loads positively on each of the 30 stocks. They conclude that the returns of individual stocks are "dominated by the tone of the market." Borrowing their terminology, we can rephrase the question addressed by this section: is there a significant "tone of the market" for cryptocurrencies?
\subsection{Proportion test}
To answer these questions, we employ a rough test similar to the one Feeney and Hester (1967) use to determine whether it is useful to aggregate stocks into industries. If such an aggregation were useful, they postulate that we would expect stocks in the same industry to enter different components of the market with similar weights, i.e., for the stocks to have similar correlations to different components of market returns. Similarly, we aim to discover whether the cryptocurrencies enter the components of the cryptocurrency market returns with similar weights. We conduct a proportions test on a null hypothesis that the correlations of two cryptocurrencies with a component (i.e., their loadings with respect that component) will be of the same sign with a probability of 0.5 (with a one-sided right-tail rejection region). Since we will be limiting our analysis to the number of components that it takes to explain at least 90\% of the variation in cryptocurrency returns (5), the test is performed on the pooled sum of the 105 pairwise comparisons at a 5\% significance level (seven cryptocurrencies in pairs of two across three components: $\choose{7}{2} \times 5$. Like Feeney and Hester (1967), we will assume the signs are binomially distributed in order to use this test.
\bigbreak
In other words, accepting the null hypothesis signifies that the likelihood of any two assets having correlation coefficients of the same sign with the various components is roughly random (a 50\% chance). With the alternative hypothesis, the more frequently a pair of cryptocurrencies have correlation coefficients of the same sign with the various components, the more justified we should feel in conceptually aggregating cryptocurrencies into one investment class (hence the right-tailed rejection region). Rejecting this hypothesis would imply that an investor could reduce the risk in their portfolio by spreading out their exposure to cryptocurrencies among multiple currencies, just as one might spread out their exposure to equities among many stocks. The test yields a $z$-statistic of 1.659, which is statistically significant at the 5\% level. Consequently, we reject the null hypothesis and conclude that investing in more than one currency would provide no advantage from the perspective of building a diversified portfolio to reduce overall variance (unless, perhaps, you were to "short" one or more of the other currencies). The results of this test lead us to believe with more conviction that we are justified in aggregating cryptocurrencies into one investment class, similar to an industry grouping in equities.
\bigbreak
Although this was a rough test, we elucidate the results by providing two points of reference. We conduct the same proportions test that we did on the cryptocurrencies to two other sets of seven assets over the same time period. The first set consists of the top seven holdings of the Energy Select Sector SPDR® Fund (Bloomberg Ticker: XLE), which "seeks to provide an effective representation of the energy sector of the S\&P 500 Index." The energy sector is often described as one in which the stocks are highly correlated with one another, with banks such as Goldman Sachs and JP Morgan often making sector-wide recommendation calls (i.e., buy or sell the "energy stocks") depending on the tides of oil prices. The second set of seven assets was selected with the deliberate intent of including not only stocks of many industries but exchange-traded funds representing various industries as well. We'll refer to this set as the "basket." Tables \ref{table:6} and \ref{table:7} provide more details on the contents of both sets of assets as well as their loading results. We performed the test on each of these sets with a number of components equal to the number required to account for at least 90\% of the variance and compare the results in table \ref{table:8}, where $\sigma^2$ refers to the variation accounted for by the number of components used. Given the $p$-values of both tests are above 5\%, we accept the null hypothesis for both the energy and the basket groups.

\begin{table}[h!]
	\centering
	\begin{tabular}{cccccccc}
		\toprule
		\head{Principal Component} & \head{AAPL} & \head{GLD} & \head{HYG} &      \head{X} & \head{GE} & \head{M} & \head{MOS} \\
		\midrule
		1         &  0.825 &  0.809 & -0.459 & -0.450 & -1.074 & -0.135 &  0.227 \\
		2         &  0.057 & -0.421 & -0.185 &  0.572 & -0.486 &  0.091 & -0.193 \\
		3         &  0.017 & -0.069 &  0.017 & -0.193 & -0.036 & -0.107 & -0.393 \\
		4         & -0.207 &  0.208 & -0.024 &  0.094 & -0.050 & -0.000 & -0.088 \\
		5         &  0.086 &  0.055 &  0.122 &  0.072 &  0.017 &  0.005 & -0.039 \\
		6         & -0.046 & -0.026 &  0.081 & -0.024 & -0.066 & -0.049 &  0.037 \\
		7         & -0.006 & -0.002 &  0.013 & -0.017 & -0.013 &  0.052 & -0.004 \\
		\bottomrule
	\end{tabular}
	\caption{PCA loadings for diverse mix of stocks and exchange-traded funds: Apple Inc. (AAPL), SPDR Gold Shares ETF (GLD),  iShares iBoxx \$ High Yid Corp Bond ETF (HYG), United States Steel Corporation (X), General Electric (GE), Macy's Inc (M), Mosaic Co (MOS) }
	\label{table:5}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{cccccccc}
		\toprule
		\head{Principal Component} & \head{COP} & \head{CVX} & \head{EOG} &      \head{OXY} & \head{XOM} & \head{SLB} & \head{PSX} \\
		\midrule
		1         & -0.996 & -0.959 & -0.886 & -0.861 & -0.541 & -0.812 & -0.860 \\
		2         & -0.672 & -0.035 & -0.092 &  0.383 & -0.269 &  0.226 &  0.483 \\
		3         &  0.146 &  0.353 & -0.351 & -0.163 & -0.131 & -0.366 &  0.390 \\
		4         &  0.226 & -0.043 &  0.108 &  0.126 & -0.577 &  0.000 & -0.089 \\
		5         & -0.013 & -0.142 &  0.208 &  0.170 &  0.069 & -0.355 &  0.081 \\
		6         &  0.143 & -0.141 & -0.282 &  0.255 &  0.075 &  0.015 & -0.034 \\
		7         & -0.100 &  0.234 & -0.017 &  0.144 &  0.003 & -0.076 & -0.201 \\
		\bottomrule
	\end{tabular}
	\caption{PCA loadings for top seven holdings of the Energy Select Sector SPDR® Fund: ConocoPhillips (COP), Chevron Corporation (CVX), EOG Resources Inc (EOG), Occidental Petroleum Corporation (OXY), Exxon Mobil Corporation (XOM), Schlumberger Limited. (SLB), Phillips 66 (PSX)}
	\label{table:6}
\end{table}


\begin{table}[h!]
	\centering
	\begin{tabular}{lccc}
		\toprule
		{} &  \head{Crypto} &  \head{Energy} &  \head{Basket} \\
		\midrule
		Components &   5 &   4 &   6 \\
		$z$        &   1.659 &   1.309 &   1.247 \\
		$p$-value  &   0.049 &   0.095 &   0.106 \\
		$\sigma^2$ &   0.900 &   0.928 &   0.954 \\
		\bottomrule
	\end{tabular}
	\caption{Results of proportions test on three sets of assets}
	\label{table:7}
\end{table}

\subsection{Implications and limitations}

We have demonstrated that the likelihood of any pair of cryptocurrencies having similar-signed loadings is more statistically significant than it is for any pair of equities in the energy or basket subsets. Given this implies cryptocurrencies have a strong comovement pattern, it signifies that investors have relatively less to gain by spreading their exposure to cryptocurrencies among multiple currencies than they would by, for example, spreading their exposure to the basket of equities across multiple stocks. If we look at the loadings for the diverse mix of stocks and ETFs in \label{table:5}, one can see how, for example, an investor could capture uncorrelated variation by purchasing AAPL and GE stock. On the first component, AAPL's loading (0.825) is nearly opposite to that of GE (-1.074), with similar results on the second component (AAPL with 0.057 vs. GE with -0.486). The pair of GLD and X also demonstrate this effect. By choosing these "attractive" pairs, an investor could eliminate much of the variance represented by the second component from their portfolio. We cannot find similarly attractive pairings in the cryptocurrency loadings matrix given their high level of comovement.
\bigbreak
In this section, we have implicitly assumed that the universe of investment assets consisted solely of the set of assets for which we performed each respective proportions test. This would be an unlikely reality for most investors, at least those upon which modern portfolio theory is based. We limited the universe in this way in order to easily tie the analysis into the prior discussions on the eigendecomposition of the covariance matrix of returns for a subset of assets. In reality, however, investors in cryptocurrencies will likely hold other assets in their portfolio. In such a situation, the covariance matrix to be studied would be larger and the loadings different. However, many of the same inter-currency dynamics would hold. It is also worth mentioning that the results for the energy and basket stock groups would look significantly different if we were to use a longer time frame. For the above proportions test, we narrowed the dataset of returns for these two asset groups to the same time frame we used for cryptocurrencies. If instead, for example, we were to go back to 2012, we would reject the null hypothesis for the energy stocks and we would accept the null hypothesis for the basket stocks (with an even higher $p$-value, i.e., less significant association in loadings). This would lend more credibility to the suggestion that it is helpful to treat energy stocks as an industry grouping.

\subsection{Inter-temporal consistency of covariance matrix}

Of course, this analysis will be relevant in future years only if the covariance matrix for cryptocurrency returns remains relatively unchanged over time. It may be the case that cryptocurrencies that are highly correlated with each other in recent history may become less correlated in the future, raising the scepter of a diversification benefit to purchasing multiple cryptocurrencies. Although we cannot look into the future, we can investigate the inter-temporal consistency of the covariance matrix over the available time frame. To do this, we use a 90-sample time frame of asset returns to predict the loadings with respect to the first component in the next 90-sample period. The steps can be summarized as follows:


\begin{enumerate}
	\item Sample 90 sequential rows of unstandardized rolling-monthly returns. For the first iteration, this would give us a time series from September 6, 2015 to December 4, 2015. Standardize the returns for each cryptocurrency as we did in previous analyses.
	\item Sample 90 sequential rows of unstandardized rolling-monthly returns beginning after the first sample. For the first iteration, this would give us a time series from December 5, 2015 to March 2, 2016. Standardize these returns for each cryptocurrency as we did in the first step.
	\item Compute the PCA loadings for both the first and second time series.
	\item Fit an ordinary least squares (OLS) regression model using the loadings with respect to the first component from the first time series (and an intercept dummy variable of 1) to predict the loadings with respect to the first component from the second time series.
	\item Repeat steps 1-3 for the rest of the available dataset moving in 10-row increments. For example, for the second iteration, the first time series spans September 16, 2015 to December 14, 2015.
\end{enumerate}

If our covariance matrix were consistent between time periods, we would expect the loadings from the first time series to have some predictive utility in forecasting the loadings from the second time series. For cryptocurrencies, this turns out to not be the case. In figures \ref{fig:crypto_R2} and \ref{fig:crypto_F} we see how the $R^2$ and probability of the $F$-statistic ($Pr(F)$) values, respectively, are not only very weak but also vary dramatically.

\bigbreak
\begin{figure}[H]
	\centering
	\includegraphics[scale=.7]{crypto_R2_90D.png}
	\caption{$R^2$ for OLS predicting loadings in future period with 90-sample time series on cryptocurrencies}
	\label{fig:crypto_R2}
\end{figure}
\bigbreak

\bigbreak
\begin{figure}[H]
	\centering
	\includegraphics[scale=.7]{crypto_F_90D.png}
	\caption{$Pr(F)$ for OLS predicting loadings in future period with 90-sample time series on cryptocurrencies}
	\label{fig:crypto_F}
\end{figure}
\bigbreak

As a reference point, we perform the same exercise on the energy stocks from the prior section and show the results in figures \ref{fig:xle_R2} and \ref{fig:xle_F}.

\bigbreak
\begin{figure}[H]
	\centering
	\includegraphics[scale=.7]{xle_R2_90D.png}
	\caption{$R^2$ for OLS predicting loadings in future period with 90-sample time series on top holdings in XLE}
	\label{fig:xle_R2}
\end{figure}
\bigbreak

\bigbreak
\begin{figure}[H]
	\centering
	\includegraphics[scale=.7]{xle_F_90D.png}
	\caption{$Pr(F)$ for OLS predicting loadings in future period with 90-sample time series on top holdings in XLE}
	\label{fig:xle_F}
\end{figure}
\bigbreak


With the energy stocks, not only are the $R^2$ values generally higher, but the fact that all of the $Pr(F)$ values fall below 0.0011 demonstrates that the relationship discovered through our models is highly statistically significant. We conclude that the inter-temporal covariance matrix is not stable for cryptocurrencies.

\section{Conclusion}

\end{document}







